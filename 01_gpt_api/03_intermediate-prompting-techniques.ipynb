{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74780032",
   "metadata": {},
   "source": [
    "# Lesson 3: Intermediate Prompt Engineering and Task-Specific Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f71c31d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's set up our environment. Run this cell before proceeding with the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to setup the OpenAI client (please be patient, during initial setup)\n",
    "exec('''import importlib.util\\nimport subprocess\\nimport sys\\nimport os\\nfrom getpass import getpass\\n_client = None\\ndef check_openai_installed():\\n    if importlib.util.find_spec('openai') is None:\\n        print('OpenAI package not found. Installing...')\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\\n            'openai'])\\n        print('OpenAI package installed successfully.')\\n    else:\\n        print('OpenAI package is already installed.')\\ndef import_openai():\\n    check_openai_installed()\\n    global OpenAI\\n    from openai import OpenAI\\ndef validate_api_key(api_key):\\n    try:\\n        client = OpenAI(api_key=api_key)\\n        client.models.list()\\n        return True\\n    except Exception as e:\\n        print(f'Error validating API key: {str(e)}')\\n        return False\\ndef get_api_key():\\n    while True:\\n        api_key = os.getenv('OPENAI_API_KEY')\\n        if not api_key:\\n            print('OPENAI_API_KEY not found in environment variables.')\\n            api_key = getpass('Please enter your OpenAI API key: ')\\n        if validate_api_key(api_key):\\n            os.environ['OPENAI_API_KEY'] = api_key\\n            return api_key\\n        else:\\n            print('Invalid API key. Please try again.')\\n            os.environ.pop('OPENAI_API_KEY', None)\\ndef setup_openai_client():\\n    global _client\\n    import_openai()\\n    api_key = get_api_key()\\n    _client = OpenAI(api_key=api_key)\\n    print(f\"API Key set: {'Yes' if _client.api_key else 'No'}\")\\n    print(\\n        f\"API Key (first 5 chars): {_client.api_key[:5] if _client.api_key else 'None'}\"\\n        )\\n    return _client\\ndef get_completion(prompt, model='gpt-3.5-turbo'):\\n    global _client\\n    if _client is None:\\n        _client = setup_openai_client()\\n    messages = [{'role': 'user', 'content': prompt}]\\n    response = _client.chat.completions.create(model=model, messages=\\n        messages, temperature=0)\\n    return response.choices[0].message.content''')\n",
    "client = setup_openai_client()\n",
    "\n",
    "# ## 1. Multi-Step Text Analysis\n",
    "\n",
    "# This function performs a multi-step analysis of a given text:\n",
    "# 1. It summarizes the text in one sentence.\n",
    "# 2. It identifies three key themes based on the summary.\n",
    "# 3. It generates three thought-provoking questions for further analysis based on the themes.\n",
    "# The function uses the get_completion() method to interact with the GPT model for each step.\n",
    "# It returns a string containing the summary, themes, and questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8a3bb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_text(text):\n",
    "    # Step 1: Summarize the text\n",
    "    summary_prompt = f\"Summarize the following text in one sentence: {text}\"\n",
    "    summary = get_completion(summary_prompt)\n",
    "    \n",
    "    # Step 2: Identify key themes\n",
    "    themes_prompt = f\"Based on this summary: '{summary}', identify three key themes.\"\n",
    "    themes = get_completion(themes_prompt)\n",
    "    \n",
    "    # Step 3: Generate questions for further analysis\n",
    "    questions_prompt = f\"Given these themes: {themes}, generate three thought-provoking questions for further analysis.\"\n",
    "    questions = get_completion(questions_prompt)\n",
    "    \n",
    "    return f\"Summary: {summary}\\n\\nThemes: {themes}\\n\\nQuestions for Analysis: {questions}\"\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Climate change is one of the most pressing issues of our time. It affects every aspect of our lives, from the food we eat to the air we breathe. Scientists agree that human activities, particularly the burning of fossil fuels, are the primary cause of the rapid warming we're seeing. The consequences are far-reaching, including rising sea levels, more frequent and severe weather events, and disruptions to ecosystems worldwide. Addressing this challenge requires global cooperation and significant changes to our energy systems and lifestyles.\n",
    "\"\"\"\n",
    "\n",
    "print(analyze_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189d9ad3",
   "metadata": {},
   "source": [
    "## 2. Prompt Optimization Techniques\n",
    "This cell compares different prompting strategies for explaining AI, demonstrating how specificity, analogies, and Socratic questioning affect AI-generated responses. The 'compare_prompt_variations' function enables direct comparison of these strategies, highlighting the crucial role of prompt formulation in shaping AI outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92377f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compare_prompt_variations(base_prompt, variations):\n",
    "    results = {}\n",
    "    for name, prompt in variations.items():\n",
    "        results[name] = get_completion(prompt)\n",
    "    return results\n",
    "\n",
    "base_prompt = \"Explain the concept of artificial intelligence.\"\n",
    "variations = {\n",
    "    \"Basic\": base_prompt,\n",
    "    \"Specific\": \"Explain the concept of artificial intelligence, focusing on machine learning and neural networks.\",\n",
    "    \"Analogical\": \"Explain the concept of artificial intelligence by comparing it to human intelligence.\",\n",
    "    \"Socratic\": \"What is artificial intelligence? How does it work? What are its implications?\",\n",
    "}\n",
    "\n",
    "results = compare_prompt_variations(base_prompt, variations)\n",
    "for name, result in results.items():\n",
    "    print(f\"{name} Prompt Result:\\n{result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7c4c83",
   "metadata": {},
   "source": [
    "## 3. Task-Specific Prompting Strategies\n",
    "These cells introduce task-specific prompting strategies, which are tailored approaches for common AI tasks like text summarization, code debugging, and data interpretation. These strategies optimize the AI's performance for particular use cases, improving the quality and relevance of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b79e6e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Text Summarization\n",
    "def summarize_text(text, max_words=50):\n",
    "    prompt = f\"Summarize the following text in no more than {max_words} words:\\n\\n{text}\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "text_to_summarize = \"Your long text here...\"\n",
    "print(\"Summary:\", summarize_text(text_to_summarize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75deedb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Code Debugging\n",
    "def debug_code(code, error_message):\n",
    "    prompt = f\"\"\"\n",
    "    The following code is producing an error:\n",
    "    \n",
    "    ```\n",
    "    {code}\n",
    "    ```\n",
    "    \n",
    "    Error message: {error_message}\n",
    "    \n",
    "    Please identify the issue and suggest a fix.\n",
    "    \"\"\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "code_to_debug = \"\"\"\n",
    "def calculate_average(numbers):\n",
    "    total = sum(numbers)\n",
    "    average = total / len(numbers)\n",
    "    return average\n",
    "\n",
    "result = calculate_average([])\n",
    "print(result)\n",
    "\"\"\"\n",
    "print(\"Debugging result:\", debug_code(code_to_debug, \"ZeroDivisionError: division by zero\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ec381",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Data Interpretation\n",
    "def interpret_data(data):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following data and provide key insights:\n",
    "    \n",
    "    {data}\n",
    "    \n",
    "    Please include:\n",
    "    1. Main trends\n",
    "    2. Any anomalies\n",
    "    3. Possible implications\n",
    "    \"\"\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "data_to_interpret = \"\"\"\n",
    "Year, Sales (millions)\n",
    "2018, 1.5\n",
    "2019, 2.2\n",
    "2020, 1.8\n",
    "2021, 3.1\n",
    "2022, 3.8\n",
    "\"\"\"\n",
    "print(\"Data interpretation:\", interpret_data(data_to_interpret))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edcee02",
   "metadata": {},
   "source": [
    "## 4. Handling Ambiguity and Uncertainty\n",
    "Demonstrate strategies for prompting the model to express uncertainty or provide multiple perspectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96315a4a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_balanced_response(topic):\n",
    "    prompt = f\"\"\"\n",
    "    Provide a balanced perspective on the topic of {topic}. \n",
    "    Include:\n",
    "    1. At least two different viewpoints\n",
    "    2. Potential areas of uncertainty or debate\n",
    "    3. Any caveats or limitations in our current understanding\n",
    "    \n",
    "    Clearly indicate when you're expressing uncertainty or presenting conflicting views.\n",
    "    \"\"\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "print(generate_balanced_response(\"the impact of artificial intelligence on job markets\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1db333",
   "metadata": {},
   "source": [
    "## 5. Interactive and Iterative Prompting\n",
    "Show how to design prompts that facilitate back-and-forth interactions with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_story_generation():\n",
    "    context = \"You are a creative writing assistant helping to craft an interactive story.\"\n",
    "    story = \"Once upon a time, in a dense forest...\"\n",
    "    \n",
    "    for _ in range(3):  # Simulate 3 interactions\n",
    "        prompt = f\"\"\"\n",
    "        {context}\n",
    "        Current story: {story}\n",
    "        \n",
    "        Provide two possible continuations for this story, each no more than 50 words.\n",
    "        Label them as Option A and Option B.\n",
    "        \"\"\"\n",
    "        options = get_completion(prompt)\n",
    "        print(options)\n",
    "        \n",
    "        choice = input(\"Choose A or B to continue the story: \").upper()\n",
    "        if choice not in ['A', 'B']:\n",
    "            print(\"Invalid choice. Ending the story.\")\n",
    "            break\n",
    "        \n",
    "        continuation_prompt = f\"\"\"\n",
    "        {context}\n",
    "        Current story: {story}\n",
    "        Chosen continuation: Option {choice} from the following options:\n",
    "        {options}\n",
    "        \n",
    "        Please continue the story based on Option {choice}, adding about 50 words.\n",
    "        \"\"\"\n",
    "        continuation = get_completion(continuation_prompt)\n",
    "        story += \" \" + continuation\n",
    "        print(f\"\\nUpdated story:\\n{story}\\n\")\n",
    "\n",
    "interactive_story_generation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fbfaea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Conclusion and Exercises\n",
    "In this lesson, we've explored advanced prompt engineering techniques and task-specific strategies. These methods allow for more sophisticated interactions with AI models, enabling complex task decomposition, optimized prompts for better results, and handling of ambiguity and uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e2f45",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35190ca4",
   "metadata": {},
   "source": [
    "### Exercise 1: Task Decomposition for Complex Problem Solving\n",
    "\n",
    "Implement a function that uses task decomposition to solve a complex math word problem. The function should break down the problem into smaller steps, solve each step, and then combine the results for the final answer.\n",
    "\n",
    "1. Define a function `solve_complex_math_problem(problem)` that takes a word problem as input.\n",
    "2. Use the Chain of Thought technique to break down the problem into steps.\n",
    "3. Solve each step using the GPT model.\n",
    "4. Combine the results to get the final answer.\n",
    "5. Test your function with a complex word problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b204dac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def solve_complex_math_problem(problem):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "test_problem = \"\"\"\n",
    "A store is having a 20% off sale. Alice buys a shirt for $25 and a pair of jeans for $45. \n",
    "Bob buys two shirts and a pair of jeans. How much more does Bob spend than Alice after the discount?\n",
    "\"\"\"\n",
    "solution = solve_complex_math_problem(test_problem)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe298b",
   "metadata": {},
   "source": [
    "### Exercise 2: Optimizing Prompts for Specific Tasks\n",
    "\n",
    "Create a function that generates optimized prompts for different types of writing tasks. The function should take the task type and key information as input, and return a well-structured prompt.\n",
    "\n",
    "1. Define a function `generate_optimized_prompt(task_type, key_info)` that takes the task type (e.g., \"essay\", \"report\", \"story\") and key information as input.\n",
    "2. Implement different prompt structures for each task type.\n",
    "3. Use the key information to customize the prompt.\n",
    "4. Test your function with at least two different task types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780f407",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_optimized_prompt(task_type, key_info):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "essay_prompt = generate_optimized_prompt(\"essay\", {\"topic\": \"Climate Change\", \"word_count\": 500})\n",
    "print(\"Essay Prompt:\")\n",
    "print(essay_prompt)\n",
    "\n",
    "story_prompt = generate_optimized_prompt(\"story\", {\"genre\": \"Science Fiction\", \"main_character\": \"Time Traveler\"})\n",
    "print(\"\\nStory Prompt:\")\n",
    "print(story_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938eb03f",
   "metadata": {},
   "source": [
    "### Exercise 3: Handling Ambiguity and Uncertainty\n",
    "\n",
    "Extend the `generate_balanced_response` function to include a confidence score for different aspects of the response. The function should provide a more nuanced view of the topic by indicating the level of certainty for various claims or viewpoints.\n",
    "\n",
    "1. Modify the `generate_balanced_response` function to include confidence scores.\n",
    "2. Update the prompt to ask the model to provide confidence levels (e.g., high, medium, low) for different aspects of the response.\n",
    "3. Parse the response to extract the main points and their associated confidence levels.\n",
    "4. Present the balanced view with confidence indicators.\n",
    "5. Test your updated function with a controversial or complex topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1857c",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "def generate_balanced_response_with_confidence(topic):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test your function\n",
    "response = generate_balanced_response_with_confidence(\"the effectiveness of renewable energy sources\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
