{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdef1edd",
   "metadata": {},
   "source": [
    "# Introduction to the GPT API\n",
    "\n",
    "In this notebook, we'll explore the basics of using the OpenAI GPT API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be9452",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run this cell to setup the OpenAI client (please be patient, during initial setup)\n",
    "exec('''import importlib.util\\nimport subprocess\\nimport sys\\nimport os\\nfrom getpass import getpass\\n_client = None\\ndef check_openai_installed():\\n    if importlib.util.find_spec('openai') is None:\\n        print('OpenAI package not found. Installing...')\\n        subprocess.check_call([sys.executable, '-m', 'pip', 'install',\\n            'openai'])\\n        print('OpenAI package installed successfully.')\\n    else:\\n        print('OpenAI package is already installed.')\\ndef import_openai():\\n    check_openai_installed()\\n    global OpenAI\\n    from openai import OpenAI\\ndef validate_api_key(api_key):\\n    try:\\n        client = OpenAI(api_key=api_key)\\n        client.models.list()\\n        return True\\n    except Exception as e:\\n        print(f'Error validating API key: {str(e)}')\\n        return False\\ndef get_api_key():\\n    while True:\\n        api_key = os.getenv('OPENAI_API_KEY')\\n        if not api_key:\\n            print('OPENAI_API_KEY not found in environment variables.')\\n            api_key = getpass('Please enter your OpenAI API key: ')\\n        if validate_api_key(api_key):\\n            os.environ['OPENAI_API_KEY'] = api_key\\n            return api_key\\n        else:\\n            print('Invalid API key. Please try again.')\\n            os.environ.pop('OPENAI_API_KEY', None)\\ndef setup_openai_client():\\n    global _client\\n    import_openai()\\n    api_key = get_api_key()\\n    _client = OpenAI(api_key=api_key)\\n    print(f\"API Key set: {'Yes' if _client.api_key else 'No'}\")\\n    print(\\n        f\"API Key (first 5 chars): {_client.api_key[:5] if _client.api_key else 'None'}\"\\n        )\\n    return _client\\ndef get_completion(prompt, model='gpt-3.5-turbo'):\\n    global _client\\n    if _client is None:\\n        _client = setup_openai_client()\\n    messages = [{'role': 'user', 'content': prompt}]\\n    response = _client.chat.completions.create(model=model, messages=\\n        messages, temperature=0)\\n    return response.choices[0].message.content''')\n",
    "client = setup_openai_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c1d24",
   "metadata": {},
   "source": [
    "## 1. Basic Prompt Engineering\n",
    "Let's start with a simple example of how changing the prompt can affect the output. Adding context to the prompt can improve the accuracy and relevance of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"What is the capital of France?\"\n",
    "print(\"Basic prompt result:\")\n",
    "print(get_completion(basic_prompt))\n",
    "\n",
    "improved_prompt = \"As a knowledgeable geography expert, please tell me the capital of France and provide a brief description of its significance.\"\n",
    "print(\"\\nImproved prompt result:\")\n",
    "print(get_completion(improved_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4010cc",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Learning\n",
    "Now, let's use few-shot learning to guide the model's behavior. This is a technique where the model is provided with a few examples of the desired output before being asked to generate its own output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05903971",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = \"\"\"\n",
    "Classify the sentiment of the following reviews:\n",
    "\n",
    "Review: \"This movie was amazing!\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"I hated every minute of it.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"It was okay, I guess.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Review: \"The service at this restaurant is terrible.\"\n",
    "Sentiment:\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(few_shot_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a034059b",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought Prompting\n",
    "By providing a step-by-step breakdown of the problem, we can guide the model to a correct answer. This helps with complex reasoning tasks like word problems where we can help reduce ambiguity by clarifying the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d6b48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "cot_prompt = \"\"\"\n",
    "Solve the following word problem step by step:\n",
    "\n",
    "Problem: If a train travels 120 km in 2 hours, how far will it travel in 5 hours assuming it maintains the same speed?\n",
    "\n",
    "Step 1: Calculate the speed of the train\n",
    "Step 2: Use the speed to determine the distance traveled in 5 hours\n",
    "\n",
    "Please show your work for each step.\n",
    "\"\"\"\n",
    "\n",
    "print(get_completion(cot_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee42ec",
   "metadata": {},
   "source": [
    "## 4. System Prompts\n",
    "Now, let's use a system prompt to set the behavior of the AI assistant. System prompts are used to set the behavior of the AI assistant in a more programmatic way by offering parameters such as:\n",
    "\n",
    "- **Model**: Specifies which version of the language model to use, like `gpt-3.5-turbo` or `gpt-4`. Different models can provide varying levels of comprehension and creativity.\n",
    "- **Temperature**: Controls the randomness or creativity of the output. A lower temperature (e.g., 0) makes the output more deterministic and focused, while a higher temperature (e.g., 0.7) allows for more varied and creative responses.\n",
    "- **Role**: Sets the assistant's persona or perspective, such as being a helpful assistant, a sarcastic commentator, or an expert in a specific field.\n",
    "\n",
    "By adjusting these parameters, we can influence how the AI generates responses to better suit our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79fb527",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_completion_with_system(system_prompt, user_prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "system_prompt = \"You are a helpful assistant that always responds in rhyme.\"\n",
    "user_prompt = \"Explain how photosynthesis works.\"\n",
    "\n",
    "print(get_completion_with_system(system_prompt, user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ed1edf",
   "metadata": {},
   "source": [
    "## 5. Prompt Templates\n",
    "Creating a reusable prompt template can save time and ensure consistency across different prompts. By defining a structured format for your prompts, you can easily modify key elements without having to rewrite the entire prompt each time. This approach not only enhances efficiency but also helps maintain a uniform tone and style across various interactions, making it easier to manage and scale your prompt engineering efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def product_description_template(product_name, key_features, target_audience):\n",
    "    return f\"\"\"\n",
    "    Create a compelling product description for {product_name}. \n",
    "    Key features: {', '.join(key_features)}\n",
    "    Target audience: {target_audience}\n",
    "    \n",
    "    The description should be engaging, highlight the key features, and appeal to the target audience.\n",
    "    \"\"\"\n",
    "\n",
    "template = product_description_template(\n",
    "    \"EcoBoost 3000 Electric Bike\",\n",
    "    [\"500W motor\", \"50-mile range\", \"Lightweight aluminum frame\"],\n",
    "    \"Urban commuters aged 25-45\"\n",
    ")\n",
    "\n",
    "print(get_completion(template))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4aa6be",
   "metadata": {},
   "source": [
    "## 6. Retrieval-Augmented Generation (RAG)\n",
    "Retrieval-Augmented Generation (RAG) is often regarded as a technique specifically designed for large language models (LLMs) because it enhances their ability to generate contextually relevant responses by integrating external knowledge sources. However, the essential idea behind RAGâ€”combining retrieval of information with generative capabilitiesâ€”is applicable beyond LLMs. It emphasizes the importance of grounding responses in factual data, which can improve the accuracy and relevance of outputs in various AI applications. For this example, we'll simulate a simple RAG system using a predefined knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa996479",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base = {\n",
    "    \"Python\": \"Python is a high-level, interpreted programming language known for its simplicity and readability.\",\n",
    "    \"Machine Learning\": \"Machine Learning is a subset of AI that focuses on the development of algorithms that can learn from and make predictions or decisions based on data.\",\n",
    "    \"Neural Networks\": \"Neural Networks are a series of algorithms that aim to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.\"\n",
    "}\n",
    "\n",
    "def rag_prompt(query):\n",
    "    relevant_info = knowledge_base.get(query, \"No specific information found in the knowledge base.\")\n",
    "    return f\"\"\"\n",
    "    Using the following information from our knowledge base:\n",
    "    '{relevant_info}'\n",
    "    \n",
    "    Please answer the user's question: '{query}'\n",
    "    Provide additional context or examples if appropriate.\n",
    "    \"\"\"\n",
    "\n",
    "user_query = \"What is Python?\"\n",
    "print(get_completion(rag_prompt(user_query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb015d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Conclusion\n",
    "We've explored various prompting techniques, from basic prompt engineering to more advanced concepts like few-shot learning, chain-of-thought prompting, system prompts, prompt templates, and a simple example of RAG. These techniques demonstrate the flexibility and power of large language models, allowing us to achieve complex tasks without traditional model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3c3dd",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Let's practice and expand on the prompting techniques we've learned with these exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce0795",
   "metadata": {},
   "source": [
    "### Exercise 1: System Prompts and Role-Playing\n",
    "\n",
    "Create a system prompt that instructs the AI to act as a historical figure. Then, write a user prompt asking a question related to that figure's expertise or time period. Use the `get_completion` function to generate a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807d04d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def historical_figure_chat(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = get_completion(messages)\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "system_prompt = \"You are Leonardo da Vinci, the Renaissance polymath. Respond as if you are living in the late 15th century, with knowledge of art, science, and engineering of that time.\"\n",
    "user_prompt = \"What are your thoughts on the future of human flight?\"\n",
    "\n",
    "response = historical_figure_chat(system_prompt, user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec1e5d",
   "metadata": {},
   "source": [
    "Now it's your turn! Choose a different historical figure and create a system prompt for them. Then, ask them a relevant question using the `historical_figure_chat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a231ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: system_prompt = \"You are [historical figure]. ...\"\n",
    "#       user_prompt = \"...\"\n",
    "#       response = historical_figure_chat(system_prompt, user_prompt)\n",
    "#       print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176b7b6",
   "metadata": {},
   "source": [
    "### Exercise 2: Chain-of-Thought Prompting for Problem Solving\n",
    "\n",
    "Use chain-of-thought prompting to solve a multi-step problem. Create a prompt that guides the model through the problem-solving process step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228d905",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def solve_problem_with_cot(problem_statement):\n",
    "    prompt = f\"\"\"\n",
    "    Solve the following problem step by step:\n",
    "    \n",
    "    Problem: {problem_statement}\n",
    "    \n",
    "    Please follow these steps:\n",
    "    1. Identify the key information given in the problem.\n",
    "    2. Determine what needs to be calculated.\n",
    "    3. Choose the appropriate formula or method to solve the problem.\n",
    "    4. Perform the calculations, showing your work.\n",
    "    5. Check if the result makes sense in the context of the problem.\n",
    "    6. State the final answer clearly.\n",
    "    \n",
    "    Show your work for each step and explain your reasoning.\n",
    "    \"\"\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "# Example usage:\n",
    "problem = \"A car travels 150 miles in 2.5 hours. What is its average speed in miles per hour?\"\n",
    "solution = solve_problem_with_cot(problem)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410ca80",
   "metadata": {},
   "source": [
    "Now, create your own multi-step problem and use the `solve_problem_with_cot` function to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ad3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: your_problem = \"...\"\n",
    "#       your_solution = solve_problem_with_cot(your_problem)\n",
    "#       print(your_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b298072",
   "metadata": {},
   "source": [
    "### Exercise 3: Few-Shot Learning for Text Classification\n",
    "\n",
    "Create a few-shot learning prompt to classify text into sentiment categories: \"Positive\", \"Negative\", or \"Neutral\". Provide examples for each category, then test your prompt with new text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73dee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_classifier(text_to_classify):\n",
    "    prompt = f\"\"\"\n",
    "    Classify the sentiment of the following texts as Positive, Negative, or Neutral.\n",
    "\n",
    "    Text: \"I love this product! It's amazing.\"\n",
    "    Sentiment: Positive\n",
    "\n",
    "    Text: \"This movie was terrible. I want my money back.\"\n",
    "    Sentiment: Negative\n",
    "\n",
    "    Text: \"The weather is cloudy today.\"\n",
    "    Sentiment: Neutral\n",
    "\n",
    "    Text: \"I can't believe how great this restaurant is!\"\n",
    "    Sentiment: Positive\n",
    "\n",
    "    Text: \"I'm feeling quite indifferent about the whole situation.\"\n",
    "    Sentiment: Neutral\n",
    "\n",
    "    Text: \"{text_to_classify}\"\n",
    "    Sentiment:\n",
    "    \"\"\"\n",
    "    return get_completion(prompt)\n",
    "\n",
    "# Example usage:\n",
    "text_sample = \"I'm disappointed with the service I received.\"\n",
    "classification = sentiment_classifier(text_sample)\n",
    "print(f\"Text: {text_sample}\")\n",
    "print(f\"Classification: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1453598",
   "metadata": {},
   "source": [
    "Now, test the sentiment classifier with at least three more text samples of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6503e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: sample1 = \"...\"\n",
    "#       print(f\"Text: {sample1}\")\n",
    "#       print(f\"Classification: {sentiment_classifier(sample1)}\")\n",
    "#       \n",
    "#       # Repeat for sample2 and sample3"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
